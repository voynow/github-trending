{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import repo_chat.git2vectors as git2vectors\n",
    "from repo_chat.chat_utils import MultiQueryEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-gpt-3.5-turbo in organization org-Md8tvfiOhchtbekg8ebz8Fo4 on tokens per min. Limit: 90000 / min. Current: 88239 / min. Contact us through our help center at help.openai.com if you continue to have issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\voyno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\process.py\", line 243, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"C:\\Users\\voyno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\process.py\", line 202, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"C:\\Users\\voyno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\process.py\", line 202, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py\", line 202, in run_query\n    self.get_vectorstore, query, self.runs_per_query).evaluate()\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py\", line 188, in evaluate\n    response = self.score_response(chain, self.query)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py\", line 176, in score_response\n    response = chain.chat(query)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py\", line 121, in chat\n    return self.manage_workflow(query, context_validator, run_query)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py\", line 101, in manage_workflow\n    answer = self.iterate_through_queries(\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py\", line 88, in iterate_through_queries\n    answer = self.process_query(\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py\", line 72, in process_query\n    val_resp = context_validator(chain_inputs)['text']\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 140, in __call__\n    raise e\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chains\\base.py\", line 134, in __call__\n    self._call(inputs, run_manager=run_manager)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 69, in _call\n    response = self.generate([inputs], run_manager=run_manager)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chains\\llm.py\", line 79, in generate\n    return self.llm.generate_prompt(\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py\", line 142, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py\", line 90, in generate\n    raise e\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py\", line 82, in generate\n    results = [\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chat_models\\base.py\", line 83, in <listcomp>\n    self._generate(m, stop=stop, run_manager=run_manager)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chat_models\\openai.py\", line 293, in _generate\n    response = self.completion_with_retry(messages=message_dicts, **params)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chat_models\\openai.py\", line 254, in completion_with_retry\n    return _completion_with_retry(**kwargs)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 289, in wrapped_f\n    return self(f, *args, **kw)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 379, in __call__\n    do = self.iter(retry_state=retry_state)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 325, in iter\n    raise retry_exc.reraise()\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 158, in reraise\n    raise self.last_attempt.result()\n  File \"C:\\Users\\voyno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 438, in result\n    return self.__get_result()\n  File \"C:\\Users\\voyno\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py\", line 390, in __get_result\n    raise self._exception\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\tenacity\\__init__.py\", line 382, in __call__\n    result = fn(*args, **kwargs)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\langchain\\chat_models\\openai.py\", line 252, in _completion_with_retry\n    return self.client.create(**kwargs)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n    return super().create(*args, **kwargs)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n    response, _, api_key = requestor.request(\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 230, in request\n    resp, got_stream = self._interpret_response(result, stream)\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 624, in _interpret_response\n    self._interpret_response_line(\n  File \"c:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\venv\\lib\\site-packages\\openai\\api_requestor.py\", line 687, in _interpret_response_line\n    raise self.handle_error_response(\nopenai.error.RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-Md8tvfiOhchtbekg8ebz8Fo4 on tokens per min. Limit: 90000 / min. Current: 88239 / min. Contact us through our help center at help.openai.com if you continue to have issues.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m      3\u001b[0m queries \u001b[39m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mHow does this code work?\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mExplain this to me like I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm 5.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m evaluator \u001b[39m=\u001b[39m MultiQueryEvaluator(\n\u001b[0;32m     16\u001b[0m     git2vectors\u001b[39m.\u001b[39mget_vectorstore, queries, runs_per_query\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m responses \u001b[39m=\u001b[39m evaluator\u001b[39m.\u001b[39;49mevaluate()\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQueries vs Response Scores:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m evaluator\u001b[39m.\u001b[39mget_scores()\n",
      "File \u001b[1;32mc:\\Users\\voyno\\Desktop\\code\\repos\\repo-chat\\repo_chat\\chat_utils.py:209\u001b[0m, in \u001b[0;36mMultiQueryEvaluator.evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    207\u001b[0m responses \u001b[39m=\u001b[39m {}\n\u001b[0;32m    208\u001b[0m \u001b[39mwith\u001b[39;00m concurrent\u001b[39m.\u001b[39mfutures\u001b[39m.\u001b[39mProcessPoolExecutor() \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m--> 209\u001b[0m     \u001b[39mfor\u001b[39;00m query, response \u001b[39min\u001b[39;00m executor\u001b[39m.\u001b[39mmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_query, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqueries):\n\u001b[0;32m    210\u001b[0m         responses[query] \u001b[39m=\u001b[39m response\n\u001b[0;32m    211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponses \u001b[39m=\u001b[39m responses\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\process.py:567\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    562\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[39m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \u001b[39m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m    568\u001b[0m         element\u001b[39m.\u001b[39mreverse()\n\u001b[0;32m    569\u001b[0m         \u001b[39mwhile\u001b[39;00m element:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[0;32m    606\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    607\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 608\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39;49mpop()\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    609\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    610\u001b[0m         \u001b[39myield\u001b[39;00m fs\u001b[39m.\u001b[39mpop()\u001b[39m.\u001b[39mresult(end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    437\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Rate limit reached for default-gpt-3.5-turbo in organization org-Md8tvfiOhchtbekg8ebz8Fo4 on tokens per min. Limit: 90000 / min. Current: 88239 / min. Contact us through our help center at help.openai.com if you continue to have issues."
     ]
    }
   ],
   "source": [
    "# run multiple queries n times each\n",
    "\n",
    "queries = [\n",
    "    \"How does this code work?\",\n",
    "    \"Explain this to me like I'm 5.\",\n",
    "    \"What is this for?\",\n",
    "    \"What does this do?\",\n",
    "    \"How would a beginner understand this?\",\n",
    "    \"What is the point of the .env file?\",\n",
    "    \"How do I use smol to create a new project?\",\n",
    "    \"Can my junior dev create a website?\",\n",
    "\n",
    "]\n",
    "\n",
    "evaluator = MultiQueryEvaluator(\n",
    "    git2vectors.get_vectorstore, queries, runs_per_query=10)\n",
    "responses = evaluator.evaluate()\n",
    "\n",
    "print(\"Queries vs Response Scores:\")\n",
    "evaluator.get_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
