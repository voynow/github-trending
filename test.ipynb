{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = '''\n",
    "class Block(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        template: str,\n",
    "        role: str = \"user\",\n",
    "        model_name: str = \"gpt-3.5-turbo-16k\",\n",
    "        temperature: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Super simple interface for chat-like GPT completions\n",
    "\n",
    "        template (str): The template to use for the model\n",
    "        role (str, optional): The role of the user. Defaults to \"user\".\n",
    "        model_name (str, optional): The model name to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): The temperature to use. Defaults to 0.2.\n",
    "        stream (bool, optional): Whether to stream the output or not. Defaults to False.\n",
    "        \"\"\"\n",
    "        self.template = template\n",
    "        self.input_variables = self.get_input_variables()\n",
    "        self.message = {\"role\": role, \"content\": None}\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.logs = []\n",
    "\n",
    "    def get_input_variables(self):\n",
    "        return re.findall(r\"\\{(\\w+)\\}\", self.template)\n",
    "\n",
    "    def log(self, request, response, response_time=None):\n",
    "        log_entry = {\n",
    "            \"inputs\": request,\n",
    "            \"response\": response,\n",
    "        }\n",
    "        if response_time:\n",
    "            log_entry[\"response_time\"] = response_time\n",
    "        self.logs.append(log_entry)\n",
    "\n",
    "    def create_completion(\n",
    "        self, inputs: Dict[str, Any]\n",
    "    ) -> Union[Dict[str, Any], Generator]:\n",
    "        \"\"\"Create a GPT completion\"\"\"\n",
    "        self.message[\"content\"] = self.template.format(**inputs)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model_name,\n",
    "            messages=[self.message],\n",
    "            temperature=self.temperature,\n",
    "            stream=self.stream,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute(self, inputs: str) -> Union[str, Generator]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def display(self, content) -> None:\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs) -> Union[str, Generator]:\n",
    "        inputs = {}\n",
    "        if args:\n",
    "            inputs = {key: value for key, value in zip(self.input_variables, args)}\n",
    "        if kwargs:\n",
    "            inputs.update(kwargs)\n",
    "        return self.execute(inputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from llm_blocks import blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Here is {symbol} from {reponame}:\n",
    "{code}\n",
    "\n",
    "Can you convert this python code to pseudocode? Focus on the high-level logic not syntax or naming. Be concise.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudocode for the given Python code:\n",
      "\n",
      "1. Define a class named \"Block\" that inherits from the ABC class.\n",
      "2. Initialize the Block class with the following parameters: template, role, model_name, and temperature.\n",
      "3. Set the template, input_variables, message, model_name, temperature, and logs attributes of the Block class.\n",
      "4. Define a method named \"get_input_variables\" that returns a list of input variables extracted from the template.\n",
      "5. Define a method named \"log\" that takes request, response, and response_time as parameters and appends a log entry to the logs attribute.\n",
      "6. Define a method named \"create_completion\" that takes inputs as a dictionary and creates a GPT completion using the OpenAI API.\n",
      "7. Define abstract methods named \"execute\" and \"display\" that need to be implemented by subclasses.\n",
      "8. Define the \"__call__\" method that takes arguments as inputs and executes the \"execute\" method with the provided inputs.\n",
      "9. If arguments are provided, create a dictionary of inputs using the input_variables and args.\n",
      "10. If keyword arguments are provided, update the inputs dictionary with the keyword arguments.\n",
      "11. Return the result of executing the \"execute\" method with the inputs."
     ]
    }
   ],
   "source": [
    "block = blocks.StreamBlock(template=template)\n",
    "response_generator = block(symbol=\"llm_blocks/blocks.py Block\", reponame=\"llm-blocks\", code=code)\n",
    "block.display(response_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inputs': {'symbol': 'llm_blocks/blocks.py Block',\n",
       "   'reponame': 'llm-blocks',\n",
       "   'code': '\\nclass Block(ABC):\\n    def __init__(\\n        self,\\n        template: str,\\n        role: str = \"user\",\\n        model_name: str = \"gpt-3.5-turbo-16k\",\\n        temperature: float = 0.1,\\n    ):\\n        \"\"\"\\n        Super simple interface for chat-like GPT completions\\n\\n        template (str): The template to use for the model\\n        role (str, optional): The role of the user. Defaults to \"user\".\\n        model_name (str, optional): The model name to use. Defaults to \"gpt-3.5-turbo\".\\n        temperature (float, optional): The temperature to use. Defaults to 0.2.\\n        stream (bool, optional): Whether to stream the output or not. Defaults to False.\\n        \"\"\"\\n        self.template = template\\n        self.input_variables = self.get_input_variables()\\n        self.message = {\"role\": role, \"content\": None}\\n        self.model_name = model_name\\n        self.temperature = temperature\\n        self.logs = []\\n\\n    def get_input_variables(self):\\n        return re.findall(r\"\\\\{(\\\\w+)\\\\}\", self.template)\\n\\n    def log(self, request, response, response_time=None):\\n        log_entry = {\\n            \"inputs\": request,\\n            \"response\": response,\\n        }\\n        if response_time:\\n            log_entry[\"response_time\"] = response_time\\n        self.logs.append(log_entry)\\n\\n    def create_completion(\\n        self, inputs: Dict[str, Any]\\n    ) -> Union[Dict[str, Any], Generator]:\\n        \"\"\"Create a GPT completion\"\"\"\\n        self.message[\"content\"] = self.template.format(**inputs)\\n        response = openai.ChatCompletion.create(\\n            model=self.model_name,\\n            messages=[self.message],\\n            temperature=self.temperature,\\n            stream=self.stream,\\n        )\\n        return response\\n\\n    @abstractmethod\\n    def execute(self, inputs: str) -> Union[str, Generator]:\\n        pass\\n\\n    @abstractmethod\\n    def display(self, content) -> None:\\n        pass\\n\\n    def __call__(self, *args, **kwargs) -> Union[str, Generator]:\\n        inputs = {}\\n        if args:\\n            inputs = {key: value for key, value in zip(self.input_variables, args)}\\n        if kwargs:\\n            inputs.update(kwargs)\\n        return self.execute(inputs)\\n'},\n",
       "  'response': <generator object EngineAPIResource.create.<locals>.<genexpr> at 0x000002AEA43AB300>}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"chatcmpl-7pzNvAtrJiLuQqwlJTCTQ9BrPUtqR\",\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"created\": 1692625047,\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"role\": \"assistant\",\n",
      "        \"content\": \"Pseudocode for the given Python code:\\n\\n1. Define a class called \\\"Block\\\" that inherits from the ABC class.\\n2. Initialize the Block class with the following parameters: template, role, model_name, and temperature.\\n3. Set the template, input_variables, message, model_name, temperature, and logs attributes of the Block class.\\n4. Define a method called \\\"get_input_variables\\\" that returns a list of input variables extracted from the template.\\n5. Define a method called \\\"log\\\" that takes request, response, and response_time as parameters and appends a log entry to the logs attribute.\\n6. Define a method called \\\"create_completion\\\" that takes inputs as a dictionary and creates a GPT completion using the OpenAI API.\\n7. Define abstract methods called \\\"execute\\\" and \\\"display\\\" that need to be implemented by subclasses.\\n8. Define a \\\"__call__\\\" method that takes arguments and keyword arguments, converts them into a dictionary of inputs, and calls the \\\"execute\\\" method.\\n9. End of the Block class.\\n\\nNote: The pseudocode focuses on the high-level logic and does not include specific syntax or naming conventions.\"\n",
      "      },\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 527,\n",
      "    \"completion_tokens\": 234,\n",
      "    \"total_tokens\": 761\n",
      "  },\n",
      "  \"response_time\": 6.747413396835327\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "block = blocks.BatchBlock(template=template, model_name=\"gpt-3.5-turbo\")\n",
    "response = block(symbol=\"llm_blocks/blocks.py Block\", reponame=\"llm-blocks\", code=code)\n",
    "block.display(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['inputs', 'response'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.logs[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
