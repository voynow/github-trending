{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "\n",
    "import custom_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data from dataloader: 937\n"
     ]
    }
   ],
   "source": [
    "# complete rewrite of langchain git loader using parallelism\n",
    "loader = custom_loaders.TurboGitLoader(\n",
    "    clone_url=\"https://github.com/hwchase17/langchain\",\n",
    "    repo_path=\"./example_data/TurboGitLoader/\",\n",
    "    branch=\"master\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".py\")\n",
    ")\n",
    "data = loader.load()\n",
    "print(f\"Length of data from dataloader: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_content': '\"\"\"Prompt for the router chain in the multi-prompt chain.\"\"\"\\r\\n\\r\\nMULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\\\\\\r\\nGiven a raw text input to a language model select the model prompt best suited for \\\\\\r\\nthe input. You will be given the names of the available prompts and a description of \\\\\\r\\nwhat the prompt is best suited for. You may also revise the original input if you \\\\\\r\\nthink that revising it will ultimately lead to a better response from the language \\\\\\r\\nmodel.\\r\\n\\r\\n<< FORMATTING >>\\r\\nReturn a markdown code snippet with a JSON object formatted to look like:\\r\\n```json\\r\\n{{{{\\r\\n    \"destination\": string \\\\\\\\ name of the prompt to use or \"DEFAULT\"\\r\\n    \"next_inputs\": string \\\\\\\\ a potentially modified version of the original input\\r\\n}}}}\\r\\n```\\r\\n\\r\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR \\\\\\r\\nit can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\\r\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any \\\\\\r\\nmodifications are needed.\\r\\n\\r\\n<< CANDIDATE PROMPTS >>\\r\\n{destinations}\\r\\n\\r\\n<< INPUT >>\\r\\n{{input}}\\r\\n\\r\\n<< OUTPUT >>\\r\\n\"\"\"\\r\\n',\n",
       " 'metadata': {'file_path': 'langchain\\\\chains\\\\router\\\\multi_prompt_prompt.py',\n",
       "  'file_name': 'multi_prompt_prompt.py',\n",
       "  'file_type': '.py'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from getpass import getpass\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "OPENAI_API_KEY = getpass('OPENAI_API_KEY=')\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"\"An agent designed to hold a conversation in addition to using tools.\"\"\"\\r\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 1536)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_resp = embed.embed_documents(data[2].page_content)\n",
    "len(embed_resp), len(embed_resp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index testindex already exists...\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "\n",
    "index_name = 'testindex'\n",
    "pinecone.init(\n",
    "    api_key=getpass('PINECONE_API_KEY'), \n",
    "    environment=\"us-west1-gcp-free\"\n",
    ")\n",
    "\n",
    "if index_name in pinecone.list_indexes():\n",
    "    print(f\"Index {index_name} already exists...\")\n",
    "else:\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='dotproduct',\n",
    "        dimension=len(embed_resp[0])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f715026bb48f4483886d6d9b70fcf74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 100\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # first get metadata fields for this record\n",
    "    metadata = {\n",
    "        'file_name': record.metadata['file_name'],\n",
    "        'file_path': record.metadata['file_path']\n",
    "    }\n",
    "    # now we create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record.page_content)\n",
    "    # create individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\n",
    "        \"chunk\": j, \"text\": text, **metadata\n",
    "    } for j, text in enumerate(record_texts)]\n",
    "    # append these to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    # if we have reached the batch_limit we can add texts\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = embed.embed_documents(texts)\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "if len(texts) > 0:\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    embeds = embed.embed_documents(texts)\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.1,\n",
       " 'namespaces': {'': {'vector_count': 2539}},\n",
       " 'total_vector_count': 2539}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\"\"\"Chains are easily reusable components which can be linked together.\"\"\"\\r\\nfrom langchain.chains.api.base import APIChain\\r\\nfrom langchain.chains.api.openapi.chain import OpenAPIEndpointChain\\r\\nfrom langchain.chains.combine_documents.base import AnalyzeDocumentChain\\r\\nfrom langchain.chains.constitutional_ai.base import ConstitutionalChain\\r\\nfrom langchain.chains.conversation.base import ConversationChain\\r\\nfrom langchain.chains.conversational_retrieval.base import (\\r\\n    ChatVectorDBChain,\\r\\n    ConversationalRetrievalChain,\\r\\n)\\r\\nfrom langchain.chains.flare.base import FlareChain\\r\\nfrom langchain.chains.graph_qa.base import GraphQAChain\\r\\nfrom langchain.chains.hyde.base import HypotheticalDocumentEmbedder\\r\\nfrom langchain.chains.llm import LLMChain\\r\\nfrom langchain.chains.llm_bash.base import LLMBashChain\\r\\nfrom langchain.chains.llm_checker.base import LLMCheckerChain\\r\\nfrom langchain.chains.llm_math.base import LLMMathChain\\r\\nfrom langchain.chains.llm_requests import LLMRequestsChain\\r\\nfrom langchain.chains.llm_summarization_checker.base import LLMSummarizationCheckerChain\\r\\nfrom langchain.chains.loading import load_chain\\r\\nfrom langchain.chains.mapreduce import MapReduceChain\\r\\nfrom langchain.chains.moderation import OpenAIModerationChain\\r\\nfrom langchain.chains.pal.base import PALChain\\r\\nfrom langchain.chains.qa_generation.base import QAGenerationChain\\r\\nfrom langchain.chains.qa_with_sources.base import QAWithSourcesChain\\r\\nfrom langchain.chains.qa_with_sources.retrieval import RetrievalQAWithSourcesChain\\r\\nfrom langchain.chains.qa_with_sources.vector_db import VectorDBQAWithSourcesChain', metadata={'chunk': 0.0, 'file_name': '__init__.py', 'file_path': 'langchain\\\\chains\\\\__init__.py'}),\n",
       " Document(page_content='\"\"\"LangChain+ Client.\"\"\"\\r\\n\\r\\n\\r\\nfrom langchain.client.langchain import LangChainPlusClient\\r\\n\\r\\n__all__ = [\"LangChainPlusClient\"]', metadata={'chunk': 0.0, 'file_name': '__init__.py', 'file_path': 'langchain\\\\client\\\\__init__.py'}),\n",
       " Document(page_content='\"\"\"Main entrypoint into package.\"\"\"\\r\\n\\r\\nfrom importlib import metadata\\r\\nfrom typing import Optional\\r\\n\\r\\nfrom langchain.agents import MRKLChain, ReActChain, SelfAskWithSearchChain\\r\\nfrom langchain.cache import BaseCache\\r\\nfrom langchain.chains import (\\r\\n    ConversationChain,\\r\\n    LLMBashChain,\\r\\n    LLMChain,\\r\\n    LLMCheckerChain,\\r\\n    LLMMathChain,\\r\\n    PALChain,\\r\\n    QAWithSourcesChain,\\r\\n    SQLDatabaseChain,\\r\\n    VectorDBQA,\\r\\n    VectorDBQAWithSourcesChain,\\r\\n)\\r\\nfrom langchain.docstore import InMemoryDocstore, Wikipedia\\r\\nfrom langchain.llms import (\\r\\n    Anthropic,\\r\\n    Banana,\\r\\n    CerebriumAI,\\r\\n    Cohere,\\r\\n    ForefrontAI,\\r\\n    GooseAI,\\r\\n    HuggingFaceHub,\\r\\n    HuggingFaceTextGenInference,\\r\\n    LlamaCpp,\\r\\n    Modal,\\r\\n    OpenAI,\\r\\n    Petals,\\r\\n    PipelineAI,\\r\\n    SagemakerEndpoint,\\r\\n    StochasticAI,\\r\\n    Writer,\\r\\n)\\r\\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\\r\\nfrom langchain.prompts import (\\r\\n    BasePromptTemplate,\\r\\n    FewShotPromptTemplate,\\r\\n    Prompt,\\r\\n    PromptTemplate,\\r\\n)\\r\\nfrom langchain.sql_database import SQLDatabase\\r\\nfrom langchain.utilities.arxiv import ArxivAPIWrapper\\r\\nfrom langchain.utilities.google_search import GoogleSearchAPIWrapper\\r\\nfrom langchain.utilities.google_serper import GoogleSerperAPIWrapper\\r\\nfrom langchain.utilities.powerbi import PowerBIDataset', metadata={'chunk': 0.0, 'file_name': '__init__.py', 'file_path': 'langchain\\\\__init__.py'})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the main components of Langchain?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,\n",
    "    k=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Langchain has several components, including chains, agents, llms, prompts, docstore, sql_database, and utilities. Some of the chains include ConversationChain, LLMBashChain, LLMChain, LLMCheckerChain, LLMMathChain, PALChain, QAWithSourcesChain, SQLDatabaseChain, SequentialChain, SimpleSequentialChain, VectorDBQA, VectorDBQAWithSourcesChain, APIChain, LLMRequestsChain, TransformChain, MapReduceChain, OpenAIModerationChain, SQLDatabaseSequentialChain, load_chain, AnalyzeDocumentChain, HypotheticalDocumentEmbedder, ChatVectorDBChain, GraphQAChain, ConstitutionalChain, QAGenerationChain, RetrievalQA, RetrievalQAWithSourcesChain, ConversationalRetrievalChain, OpenAPIEndpointChain, and FlareChain.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.delete_index(\"testindex\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
